<div class="content content-white">
  <div class="container-fluid">
        <br class="hidden-phone">
    <div class="row-fluid">
      <div class="span6 desktop-center">
        <img src="images/david-head.jpg" class="img-circle">
      </div>
      <div class="span6">
        <br class="hidden-phone">
        <br class="hidden-phone">
        <br class="hidden-phone">
        <h1 style="font-size:600%;margin-bottom:30px;">Hi!</h1>
        <h3 style="line-height:1.4;font-weight:100;">That's me.<br>
          I'm David Rodenas, <br>
          Doctor in Computer Architecture, and <br>
          Computer Engineer.          
        </h3>
        <br>
        <br>
      </div>
    </div>
    <br class="hidden-phone">
    <div class="row-fluid">
      <div class="span12">
        <ul class="breadcrumb">
          <li><a href="#/">Home</a> <span class="divider">/</span></li>
          <li class="active">About</li>
        </ul>
      </div>
    </div>
    <div class="row-fluid">
      <div class="span12">
        <markdown>
## Why a static web-page?

I built my first homepage at '97. 
At the same time many of my friends did the same.
It was indeed the first social network.

The main motivation to build our home pages were
to give references and help to our friends. 
Common sections of all web pages were: a presentation,
links for useful things, sometimes some writing about a hobby,
and links to other friends homepages.
That was before Google; altavista, lycos, and so were useless sites,
The typical question: "I'm looking for a natural park to visit
this weekend, where can I find some information?" had
the answer: "Look to the Bob's homepage, 
he really likes go to mountain.
You can find it in my friends links."
That was very useful.

Later Google appeared and changed everything.
They created the *rank* system, and
for the first time, they converted all the data
present in homepages into a global knowledge.
Google was so useful that people stop using and updating their homepages.
Google committed suicide. 
Removing homepages the rank system started to fail, and 
Google looked for another sources.

Blogs, social networks, and so appeared,
they take the place of homepages.
But all of them are volatile,
they are not focused into a good navigation, 
they are just a bulk of topics and articles.
You can try to google now, but you will get more bulk articles.
There are curators systems online, 
but the ultimate curator is the homepage.
So..

Homepages are back!

## Bio
### Short version
I've been developer since 8 years old. 
At 13 I have learned C, created my first *"ERP"*, 
and at 16 I was already programming
with C++ object oriented programs as complex as a Ray-tracer.

I have looking for challenging projects and that
has driven me to a very amazing projects:
multi-core simulators, new programming languages, 
graph databases, real-time matching engines.

Now I'm starting a new challenge: build my own company.
I'm part time freelance and part-time building my
own technology: the Eltanin engine.

I'm really fascinated with technology, and specially
what open source has brought to us: 
amazing programming frameworks, 
big data databases,
advanced browsers, 
...
Now I'm waiting for the next big challenge :)

### Firsts steps
I was born in Barcelona 1978. 
I've been a programmer since was a child.
I started to write my firsts programs at age of 8 years
old with a ZX Spectrum and a the book "Basic para niños".
One of my firsts programs was the following:

    :::basic
    10 PRINT "COMO TE LLAMAS?"
    20 INPUT N$
    30 PRINT "HOLA ";N$;"!"
    40 GOTO 30

Years passed, and I started to do more complex programs
sequential and direct access file storages, and 
of course, little games.
I upgraded from BASIC to GW-BASIC, and eventually QBasic and
QuickBasic. The former one was amazing for me, 
because I started to use library files full of amazing
functionalities, like access to the mouse.
At this period, I remember as two great achievement 
the construction of a summation of a list of numbers of arbitrary length,
and a program that draw stars passing, just like in the Star Trek warp travel.

### Getting seasoned
At age of 13 I learned C. 
At the beginning it was really annoying, 
the code seemed to be full of semicolons,
and it was really error prone.
But it had two great characteristics:
C was awesome fast 
(so no more tricks with bulk instructions were necessary) 
and I let to use assembler easily with the `asm` keyword.
At this same age, a friend of me whose family had a grocery
asked to me for a program to register sells and compute sales,
I did it in C (with text-mode windows). My first ERP/CRM at age of 13 *XD*.

I started to use 
"The Peter Norton Programmer's Guide to the IBM PC"
to learn, and play with, assembler and MSDOS system calls.
Later I started to follow and read 
"PC Mania" and "Solo Programadores" magazines.
From these magazine I really learned more advanced programming techniques. 
I discovered VisualBasic 2.0, and
learned Pascal, C++ (object oriented programming).
I started to do advanced applications, 
including real-time 3D games or engines
(some like doom with libraries), one POV-like ray-tracer 
(I had the strange idea of do a 4D ray-tracer where
animations were defined with the 4th dimension of formulas),
Quake bots, POV landscapes,
some artificial intelligence programs including neural networks
(I built it visiting the internet newsgroup with the help of
a older sister of a friend who was studying computer science),
and many other things.

Just few months before university, 
I assisted to a short course of autonomous robotics.
It was for grownups, but I was the faster in the practices.
They gave us some robots built with car controller cards,
programmed in C, with 4 wells, and few sensors. 
We have asked to do a program to avoid obstacles
and move the robot towards a recharge station.
Instead doing regular programming, I had decided to simulate
a very simple neural network as decision making controller.
I did it so fast that they proposed me another challenge:
make the robot to find the exit of a labyrinth following
the rule of the right hand. They were astonished when
I achieved it in just 5 minutes: I just had modified 
some constants of the neural network 
to make the robot had a *desire* of touching
a wall in the right side.

### University
The computer engineering degree was like an *eat all you* can for me.
So many subjects and so few credits. But there was more than this,
I also discovered "el Casal de l'estudiant", where most of student
associations are present. 
I started to learn the basis of most of computer theory,
including validation, formal theorems, Turing like stuff,
grammars, entity modeling, caches and hierarchies, 
operating system architectures, drivers, networks, machine learning,
programming models, parallelism and parallelism theory, vector programming, 
processor design, digital circuits, programming patterns, ...
I learned many new languages: modula 2, lisp, clips, prolog, miranda 
(in fact I discovered that I was really good in functional programming), 
SQL (many dialects, including PL/SQL and HostedSQL), java, vhdl, eiffel, ocl, 
bash...

I was one board members of a Science Fiction 
association (UPCF). All associations shared the same server (which I
used to build the association home page, and when I learned
how to do CGIs the web app to manage the association). 
Later we collaborate in the foundation of LinuxUPC and we
had our own server. I boost the web app to manage the association
(including a *social network* of material sharing between members),
and I learned php, jsp, html, css, javascript, and some asp.

Around summer 1999 I had my first full time job as a programmer.
I was a core developer of a CASE development tool based in UML.
At that time UML was in beta stage
with versions prior to 1.0. 
During the development I have detected some flaws in the specification
(around the ordering of some collections of attributes) which were
reported and corrected. 
A funny thing was that when I was hired 
I reported that I had beginner Java programming skills. 
They got surprised when I started to use AWT and Swing
to create interfaces for testing tools
(from my point of view they was simple libraries).

My last year in the engineering degree 
was spent in the final
degree project: a distributed virtual machine.
This project projected my future vision of internet:
all computers working as a single working space, 
with no servers or clients, just apps running around
the world.
The idea was to create some kind of virtual machine
able to to hide the internet layer. 
Although my first intention was make a peer to peer 
virtual machine, I finally got a multiserver architecture.
This virtual machine got its own byte-code and provided
communication through message passing with transactions.

### Teacher and researcher
During my final degree project I debuted as 
computer teacher for older people. 
It was so amazing to teach computer science to
old people that I got devoted. 
Since then I take advantage of many opportunities
to be part time teacher. 
I taught informatics in the university, but also
writing techniques in my association.

#### Multi-Core kickoff
As result of my final degree project I was proposed
to join into an IBM/UPC research center. 
During the degree I have been observed that processors
were bloat of logic (transistors) 
to automate parallelism in the unique serial 
thread of instructions running inside, 
I have defended that more than thread per processor
could simplify the logic required to automate parallelism
and could improve the productivity (instructions/transistor/second).
In that time it was called multi-threaded processors,
nowadays it is called multi-core processors.
So I started to work with Cyclops: an experimental IBM processor,
multi-core and multi-threaded, with a standard configuration of 
32 cores and 4 threads/core.
That was my first step in my PhD. 
I discovered hard bottlenecks in the cache performance 
because the interference between threads and I figure out
how to avoid them allowing to exploit all parallelism offered
by the processor. 
It was presented at IPDPS Denver 2005 with great expectation
(I was almost freaking out, my first speech in english,
converted into hot-topic because of the keynote, and a room
full of people, many of them expecting to do hard questions).

> I started to work with Cyclops: an experimental IBM processor
> [...] of 32 cores and 4 threads/core.

#### Making easy programming in clusters
Eventually IBM stopped the Cyclops architecture to focus in 
new architectures, and I change the subject to *DSM*.
DSM stands for Distributed Shared Memory, 
it had some concepts close to my final degree project,
and was a hard research problem to resolve how to efficiently
emulate a shared memory processor in a cluster of computers.
It was an amazing and crazy project, 
we simulate shared memory from user space by capturing
segmentation fault signal. DSM was in fact a library linked
with the main binary, and slave servers. 
The crazy thing was that although main and slave were
different binaries, when a new thread was started in a slave
server, it starts with a segmentation fault synchronizing
all required stuff to execute: code and data.
Once we aligned properly libc and auxiliary libraries,
it worked like a charm.
We focused initially on high performance applications 
(like some NASA benchmarks), sadly, 
most of their performance was really poor.
I decided to give a try to Multi-Zone applications, 
applications in the half way between MPI 
(very complex applications for clusters) and OpenMP
(very easy to use in shared memory). 
It turned out that they scale very well,
with performance results competitive with MPI, 
and easier to build and test.

#### So many cores, so make them special
Some day magic words appeared: CellBE Processor
(cell does not relates to biological cells).
It was a processor designed by IBM for Sony and Toshiba.
It is the current processor of the Playstation 3, but
at that moment was top secret 
(and a simplified version present in the XBox 360).
So one day arrive to my hands an 
"only for your eyes document": full specs of the first
design for CellBE Processor. 
It was awesome, a processor a hundred of times faster than
any other processor at that time, a supercomputer on a chip!
But it came with a price: it was so 
hard to use that only few people could extract their potential.
So the challenge was the following: 
to design a programming framework to ease their programming.
I try to use an approach similar to the previous project.
The problem was very close to clusters due to the chip 
itself was a cluster: 9 processors of two types, 
each with their own memory, connected through a network
(in fact there are 8 extra processors just for program data movement).
Initial specs shown one bit that would be very useful to
emulate one single memory space (shared memory), 
but it was sacrificed in order to boost speed.
So, I stated that add again this bit 
(or some other kind of support) would ease so
much the programming that although the potential speed would be
lower, the programmer will be able to develop better programs,
obtaining a better performance.
And I started to make my point.

> It was awesome [...] a supercomputer on a chip!
> But it came with a price: it was so hard to use
> that only few people could extract their potential.

I started to develop a modular simulator for the Cell,
just to prove that easing the programming of the processor,
would improve the performance of the average programmer.
That simulator was a full success, 
other colleagues join me in their construction,
and finally it even had became the official simulator
of an European research project 
(participated by some companies, with ARM among them).
But in the other hand, IBM lose the interest for 
their own CellBE processor when Sony decided to 
change to other architecture. 
At that moment, GPGPUs stormed the marked,
they offered very close performances, but
with a simpler programming environments.

#### Streams: from one core to another
That was great, one day some big players decided
to join efforts to get ready for the next 10 years.
The number of cores would double each 18 months,
having more cores of the same type would not help,
so they would have specialized cores. 
But, how the average programmer can use a multi-core
with many kinds of cores?
IBM (with their current CellBE), Philips (and their
spin-offs NXP and SiliconHive), Nokia, and 
STMicroelectronics created a research project to
create a programming model for those kind of architectures,
specialised in streaming programming.

> How the average programmer can use a multi-core
> with many kinds of cores?

I presented (and created a prototype) a programming model, 
based in OpenMP concepts, to create parallel streaming 
applications based in plain C programs.

    :::c
    /* example of the acotes programming model that I have designed */
    #pragma acotes taskgroup
    {
      while (16 == fread(readbuf, 4, 16, in_file)) 
      {
          #pragma acotes task input(readbuf) output(qd_buf) firstprivate(qd_conf)
          fm_quad_demod (&qd_conf, readbuf, &qd_buf);
          #pragma acotes task input(qd_buf) output(band11) firstprivate(lp11_conf)
          ntaps_filter_ffd (&lp11_conf, 1, qd_buf, band11);
          #pragma acotes task input(qd_buf) output(band12) firstprivate(lp12_conf)
          ntaps_filter_ffd (&lp12_conf, 1, qd_buf, band12);
          #pragma acotes task input(qd_buf) output(band21) firstprivate(lp21_conf)
          ntaps_filter_ffd (&lp21_conf, 1, qd_buf, band21);
          #pragma acotes task input(qd_buf) output(band22) firstprivate(lp22_conf)
          ntaps_filter_ffd (&lp22_conf, 1, qd_buf, band22);
          #pragma acotes task input(band11, band12) output(res_1)
          subctract (band11, band12, res_1);
          #pragma acotes task input(band21, band22) output(res_2)
          subctract (band21, band22, res_2);
          #pragma acotes task input(res_1, res_2) output(ffd_buf)
          multiply_square (res_1, res_2, ffd_buf);
          #pragma acotes task input(qd_buf1) output(band2) firstprivate(lp2_conf)
          ntaps_filter_ffd (&lp2_conf, 8, qd_buf, band2); 
          #pragma acotes task input(ffd_buf) output(band3) firstprivate(lp3_conf)
          ntaps_filter_ffd (&lp3_conf, 8, ffd_buf, band3);
          #pragma acotes task input(band2, band3) output(out1, out2)
          stereo_sum (band2, band3, &out1, &out2);
          #pragma acotes task input(out1, out2) private(result)
          {
            result[0] = trunc_and_norm (out1);
            result[1] = trunc_and_norm (out2);
            fwrite (result, sizeof(short), 2, out_file);
          }
      }
    }

This was my favorite example. 
It comes from a FMradio example extracted from GNUradio
prepared by ST-microelectronics and Nokia. 
I prepared it as a simple serial program 
and encapsulated some functionalities.
Once it was done, to prove that my compiler and environment 
were valid (and get good performance) I add the `#pragma` statements.
It scaled very well.
The best of it is that if you remove or
ignore all `#pragma` statements the program still works as a regular
program.
If you use my compiler it will understand `#pragma` statements,
will break the code, will analyse dependencies between tasks,
and will create a program able to run each task in different cores
and communicate them through streams, with no need of shared memory.
Well, I'm really proud of it, 
specially because it construct safe programs,
it is very difficult to break the program just adding wrong pragmas,
and it is even more safe than many other streaming frameworks
(because the compiler computes stream connections automatically
and it ensures that always will be correct input/output ratios
between tasks, have you have noticed in some programs 
audio/video getting out of sync?).

#### CUDA: performance unleashed.
NVIDIA one day presented CUDA: a programming framework for
massive-multi-core architectures with distributed memory.
By 2010 I decided to put an end to my thesis.
There was still some things about multi-zone applications
and distributed memory that needs to be demonstrated.
I moved to another university and research group,
unfortunately without multi-core supercomputers,
but I managed to use GPGPUs to finish my research.
CUDA (like OpenCL) is a really simple programming model
that allows an average user to get an average performance
of the hundreds or thousands of cores presents in the GPU,
but that was not enough for me.
I decided to transform some new applications 
to multi-zone, and I get very impressive results
(performance increased up to 366x times using 40€ GPUs).
But I decided that that was not enough:
anyone should be able to do these kind of transformations.
So I built a pseudo-math notation that allows to adapt
programs using simple loops transformations.
Finally I got it, a simple way to transform and make
programs scalable in thousands of cores of many kinds.

### Smart data
In the second half of the 2008 I join to the
Data Management group of the UPC. 
They had built DEX, one of the firsts graph 
oriented databases.
Nowadays it is common to listen about NoSQL,
but at that time, just thinking in a database
focused in relationships, not in rows of data,
was amazing.

One of my firsts task in this group was go to
the IBM office at Toronto Canada and collaborate
with the group of DB2 RAS/PD 
(Reliability, Availability, Serviceability and Problem Determination).
DB2 is a really large piece of engineering,
its huge. There are thousands of engineers working
in it, and, some times, although the large number of
quality controls presents, some defect can rise to the surface.
When a DB2 customer has a problem, IBM has to diagnose
it and solve it (if it was a real DB2 problem). 
Usually it is some kind of misconfiguration, 
but IBM takes all efforts to make sure that it is not a DB defect.
And this is very expensive.
The idea was: use DEX to deal with all relationships
inside the code of DB2 itself and relate them to the
behaviour. 
The idea was to save time, and let DEX do the large
part of the task to spot the possible origin
(try to find one line of code between millions
and thousands of versions).
I built two prototypes. 
One did analysis between customer configuration,
related them to detected defaults, and computed
which configurations would be more prone to present
problems, and which minimal changes may be done to
have a more stable configuration.
The second prototype related stack traces,
memory dumps, and code repositories. 
It made very easy to follow *threads* to
find part of the code. 
One very successful application was to spot
deadlocks. Because it transforms all stack
traces into a graph, I just have used a common
algorithm to find cycles. With a little of coloring
it even made easier to find possible origins of the 
deadlock.
The most challenging of all this project
was that there was no such thing of unique
id, the domain was really complex.

After my experience at Toronto I came back
to Barcelona and I started to work inside of the DEX core.
It was a extreme advanced piece of technology
(it is in fact),
there were a lot advanced techniques already applied
to squeeze every second of performance. 
My tasks were first to add some functionalities to the core
and second, make it scale in multi-core multi-processor environments.
My favorite added functionality was to add the capacity
to process regular expressions. 
I was asked to not use any library, so I get back all the
theory about finite automata. 
In just three weeks I had implemented all the steps required
to build a pseudo-code to process as fast as possible an string.
It was amazing to see how so such simple algorithms were 
able to find such ingenious solutions.
In the other hand, as seasoned expert in parallel processing,
I design an API to ease programming parallel algorithms in Java
and I make DEX scale to take advantage of all cores available
in the machine. 
It was a huge tasks, I had to study carefully all functionalities
and determine which tasks could be executed concurrently and how.

### Entrepreneur
By 2011 I have founded my own company with other two colleagues.
One of our firsts assignments was to rebuild a web page of a
pharmacy trade company. Once we have resolved the problem of visibility
in Google, they wanted a simple way to maintain the list of
pharmacies to sell. I solved it connecting the web page
against a JSONP generated from a Google Spreadsheet. 
Update the database was as easy as modify a spreadsheet.

During this period I continue investigating big data,
specifically graph databases. 
I learned Neo4j and OrientDB, Gremlin and Blueprints.
For the FOSDEM 2012 I prepared a benchmark to evaluate
graph databases. 
The benchmark was based in the movielens movie recommender.
There was some benchmarks performed with small database sizes
(around 1M of recommendation), but I wanted to use the full
database.
The main query was to make a recommendation for a user based in
the ratings of that user, and the ratings from users with similar
common ratings. 
From the results, 
the SQL query was really hard, it took 9 hours make one recommendation
with the small database. 
I optimized the SQL query, using helper structures, and forcing the query
planning, and I get the same query running under 1 second even
in the large data-set. With Neo4J the execution time of the query
was under 0.1 seconds.

> The SQL query was really hard, it took 9 hours [...]
> I optimized the SQL query [...]
> and I get the same query running under 1 second.
> With Neo4J the execution time of the query was under 0.1 seconds.

In 2012 I did the job of the kind that I like:
a really big challenge. 
A German start-up, Roomsurfer, required a Facebook user matching 
to work in real-time. 
The numbers should be around 300000 users, 
each user with more than 1000 attributes,
and as soon as a new user would sign up in the platform,
a recommendation of the best matching should be presented in real-time.
The first solution proposed by them was to compute everything in batch,
make clusterization, and when a new user comes, use clusters to have
a first approximation of recommendation.
The big problem was the cost: each user has to be compared against
all other users. In other words, if you have 10x times more users,
your costs are 100x times higher (10x * 10x).
Using my data management experience, and my experience with supercomputers,
I manage to do the job, reducing the time of the matching of one
user to almost no time, so, now, when a new user is added it usually
takes 1 second of preprocessing and 
just 0.1 seconds for each list of recommendations (ordered by each
user relevance).

At the same time that I had built the graph benchmark with recommendation
I start to plan a recommendation engine based on social networks profiles.
At beginning of 2013 I deployed at Heroku an alpha version of this
project: [Eltanin](http://eltanin-eye.com). 
Eltanin is a product recommendation engine which is able to learn
what makes special each product to each user.
The current version is able to connect automatically to Facebook,
get the information, store it, and compare the profile of each
user with their product preferences, detect which traits
are relevant, and predict recommendations. 
It is able to give recommendations even for new users and products
that have never bought or sold.
Now Eltanin is already in beta stage at [Heroku](http://heroku.com/addons/eltanin).

### The Future
Nowadays are amazing. New technologies emerges everyday,
new opportunities open, new services are possible.
We already have supercomputers in our pockets,
we can access to virtually any information at any point of the planet.
All these opens endless opportunities and challenges. 
And I'm expecting the next great challenge.
I hope to update this web regularly, and adding one project or one
resource every week.

> I'm expecting the next great challenge.

        </markdown>
      </div>
    </div>
  </div>
</div>
